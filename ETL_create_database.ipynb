{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #non-destructive copy\n",
    "    alt_titles = {}\n",
    "    alternate_title_keys = ['Also known as', 'Arabic', 'Cantonese', 'Chinese','French','Hangul','Hebrew','Hepburn',\n",
    "                           'Japanese','Literally','Mandarin','McCune-Reischauer','Original title','Polish',\n",
    "                           'Revised Romanization','Romanized','Russian','Simplified','Traditional','Yiddish']\n",
    "    for key in alternate_title_keys:\n",
    "        if key in movie:  # Add alternate title to dictionary and then remove from movie dict object\n",
    "            alt_titles[key]=movie[key]\n",
    "            movie.pop(key)\n",
    "    \n",
    "    if len(alt_titles)>0:\n",
    "        movie['alt_titles']=alt_titles\n",
    "\n",
    "    # merge column names that are similar together for consistency\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name('Adaptation by', 'Writer(s)')          # old column name, new column name\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "        \n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "\n",
    "def extract_transform_load(wfile, kfile, rfile):\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    kaggle_metadata = pd.read_csv(kfile, low_memory=False)\n",
    "    ratings = pd.read_csv(rfile)\n",
    "\n",
    "    # Open and read the Wikipedia data JSON file.\n",
    "    with open(wfile, mode='r') as file:\n",
    "        wraw = json.load(file)\n",
    "    \n",
    "    # Write a list comprehension to filter out TV shows.\n",
    "    wmovies = [movie for movie in wraw if 'No. of episodes' not in movie]\n",
    "\n",
    "    # Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    # and call the clean_movie function on each movie.\n",
    "    clean_movies = [clean_movie(movie) for movie in wmovies]\n",
    "\n",
    "    # Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "\n",
    "    # Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    try:\n",
    "        wiki_movies_df['imdb_id']=wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df['imdb_id'].drop_duplicates()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    #  Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    wiki_columns_to_keep=[col for col in wiki_movies_df.columns if wiki_movies_df[col].isnull().sum()<len(wiki_movies_df)*0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "    # Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office=wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "    # Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office=box_office.apply(lambda x:' '.join(x) if type(x)==list else x)\n",
    "    # replace values listed as ranges with just single number\n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "\n",
    "    # Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    # Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'    \n",
    "\n",
    "    # Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million, remove $ and million; convert to float; return value\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|\\s|[a-zA-z]', '', s)  # also takes care of US$ \n",
    "            value = float(s) * 10**6\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion, remove $ and billion; convert to float; return value\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|\\s|[a-zA-z]' , '', s)\n",
    "            value = float(s) * 10**9\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###, remove $ and commas; convert to float; return value\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|,' , '', s)\n",
    "            value = float(s)\n",
    "            return value\n",
    "        \n",
    "        # Otherwise return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    # Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    wiki_movies_df['box_office']=box_office.str.extract(f'({form_one}|{form_two})',flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)  # drop old column with the string values\n",
    "    \n",
    "    # Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    budget = wiki_movies_df['Budget'].dropna().map(lambda x: ' '.join(x) if type(x)==list else x)\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)  # fix entries in the form of a range\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '', regex=True)\n",
    "    wiki_movies_df['budget']=budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    #wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "\n",
    "    # Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    release_date=wiki_movies_df['Release date'].dropna().apply(lambda x:' '.join(x) if type(x)==list else x)\n",
    "    date_form1 = r'[a-zA-Z]+\\s\\d{1,2},\\s\\d{4}'  # Format: Month Day, Year\n",
    "    date_form2 = r'\\d{4}.\\d{2}.\\d{2}'   # Format: YYYY-MM-DD\n",
    "    date_form3 = r'^[a-zA-Z]+\\s\\d{4}'  # Format: Month Year\n",
    "    date_form4 = r'^\\d{4}[\\s\\n]'  # Format: Year\n",
    "    wiki_movies_df['release_date']=pd.to_datetime(release_date.str.extract(f'({date_form1}|{date_form2}|{date_form3}|{date_form4})')[0], \\\n",
    "        infer_datetime_format=True)\n",
    "    #wiki_movies_df.drop('Release date', axis=1, inplace=True)\n",
    "\n",
    "    # Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m') # Convert h and m to the groups desired\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['running_time']=running_time_extract.apply(lambda row: row[0]*60+row[1] if row[2]==0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "     \n",
    "    # 2. Clean the Kaggle metadata.\n",
    "    # Per recommendation, only keep movies where 'adult' field is False. Then drop adult column since all will be false.\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True' #Convert video column to boolean.\n",
    "    # Convert numeric columns. Use errors='raise' to be alerted to anything that doesn't convert.\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    # Convert string date to datetime\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "\n",
    "    \n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    # drop title_wiki, release_date_wiki, Language, and production company(s) columns\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column]=df.apply(lambda row: row[wiki_column] if row[kaggle_column]==0 else row[kaggle_column], axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "\n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime','running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle','budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue','box_office')\n",
    "\n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    movies_cols_to_keep = ['imdb_id','id','tagline', 'belongs_to_collection',  \n",
    "        'url', 'year', 'imdb_link', 'Based on', 'Starring', 'Cinematography',\n",
    "       'Country', 'Director', 'Distributor',\n",
    "       'Editor(s)', 'Composer(s)', 'Producer(s)', 'Writer(s)', \n",
    "       'budget_kaggle', 'genres', 'homepage', \n",
    "       'original_language', 'overview', 'popularity',\n",
    "       'poster_path', 'production_companies', 'production_countries',\n",
    "       'release_date_kaggle', 'revenue', 'runtime', 'spoken_languages',\n",
    "       'status', 'title_kaggle', 'vote_average',\n",
    "       'vote_count']\n",
    "    movies_df = movies_df[movies_cols_to_keep]\n",
    "\n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    cols_renamed={'budget_kaggle':'budget', 'id':'kaggle_id', 'title_kaggle':'title' }\n",
    "    movies_df = movies_df.rename(columns=cols_renamed)\n",
    "    \n",
    "    # 9. Transform and merge the ratings DataFrame.\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1)\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "    # Deliverable 4 - create database\n",
    "    # connect with database for SQL and Postgres\n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    \n",
    "    #import the large ratings csv in chunks\n",
    "    rows_imported = 0 # for tracking during long import process\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "        print(f'importing rows {rows_imported}-{rows_imported+len(data)}...',end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported+=len(data) #increment the num of rows\n",
    "        print(f'Done. {time.time()-start_time} total seconds elapsed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the path to your file directory and variables for the three files.\n",
    "file_dir = 'data'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0-1000000...Done. 27.946850061416626 total seconds elapsed\n",
      "importing rows 1000000-2000000...Done. 56.612112283706665 total seconds elapsed\n",
      "importing rows 2000000-3000000...Done. 84.23278141021729 total seconds elapsed\n",
      "importing rows 3000000-4000000...Done. 110.79580402374268 total seconds elapsed\n",
      "importing rows 4000000-5000000...Done. 135.96974897384644 total seconds elapsed\n",
      "importing rows 5000000-6000000...Done. 161.9358344078064 total seconds elapsed\n",
      "importing rows 6000000-7000000...Done. 186.95078086853027 total seconds elapsed\n",
      "importing rows 7000000-8000000...Done. 212.55086278915405 total seconds elapsed\n",
      "importing rows 8000000-9000000...Done. 238.3777952194214 total seconds elapsed\n",
      "importing rows 9000000-10000000...Done. 262.8646864891052 total seconds elapsed\n",
      "importing rows 10000000-11000000...Done. 289.4174995422363 total seconds elapsed\n",
      "importing rows 11000000-12000000...Done. 315.8393304347992 total seconds elapsed\n",
      "importing rows 12000000-13000000...Done. 342.62993693351746 total seconds elapsed\n",
      "importing rows 13000000-14000000...Done. 368.0159418582916 total seconds elapsed\n",
      "importing rows 14000000-15000000...Done. 393.74565529823303 total seconds elapsed\n",
      "importing rows 15000000-16000000...Done. 420.46794843673706 total seconds elapsed\n",
      "importing rows 16000000-17000000...Done. 448.22893142700195 total seconds elapsed\n",
      "importing rows 17000000-18000000...Done. 475.30641412734985 total seconds elapsed\n",
      "importing rows 18000000-19000000...Done. 500.7771420478821 total seconds elapsed\n",
      "importing rows 19000000-20000000...Done. 525.9214596748352 total seconds elapsed\n",
      "importing rows 20000000-21000000...Done. 552.1994421482086 total seconds elapsed\n",
      "importing rows 21000000-22000000...Done. 577.8644707202911 total seconds elapsed\n",
      "importing rows 22000000-23000000...Done. 603.6970660686493 total seconds elapsed\n",
      "importing rows 23000000-24000000...Done. 629.288743019104 total seconds elapsed\n",
      "importing rows 24000000-25000000...Done. 654.7778222560883 total seconds elapsed\n",
      "importing rows 25000000-26000000...Done. 680.9350810050964 total seconds elapsed\n",
      "importing rows 26000000-26024289...Done. 681.5047166347504 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# 11. Set the three variables equal to the function created in D1.\n",
    "# REFACTORED CODE FOR DELIVERABLE 4\n",
    "extract_transform_load(wiki_file, kaggle_file, ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
